{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Q.1. What is a parameter?\n",
        "\n",
        "-->  In the context of feature engineering, \"parameters\" typically refer to variables within a machine learning algorithm that are learned or tuned during the training process to optimize model performance."
      ],
      "metadata": {
        "id": "Us4P4Ld4FFHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.2.What is correlation? What does negative correlation mean?\n",
        "\n",
        "--> Correlation is a statistical measure that assesses the strength and direction of the relationship between two variables.\n",
        "\n",
        "It indicates how much two variables are associated with each other.\n",
        "\n",
        "Correlation does not imply causation; just because two variables are correlated doesn't mean one causes the other.\n",
        "\n",
        "Negative Correlation:\n",
        "When two variables move in opposite directions (one increases while the other decreases), it's a negative correlation.\n"
      ],
      "metadata": {
        "id": "FdP7qUc6FM_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "--> Machine learning (ML) is a subset of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed. It involves algorithms and models that can improve with experience and data exposure, allowing them to make predictions and decisions autonomously.\n",
        "\n",
        "The main components of machine learning include data, algorithms, models, and predictions."
      ],
      "metadata": {
        "id": "ctlAjS-pHeYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.4.How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "-->  A low loss value indicates a good model because it means the model's predictions are close to the actual values. A loss function quantifies the difference between predicted and actual values, and the goal of training is to minimize this difference. A low loss value suggests the model has learned effectively and is making accurate predictions."
      ],
      "metadata": {
        "id": "oFdXFf0zHt-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.5.What are continuous and categorical variables?\n",
        "\n",
        "###Continuous Variables:\n",
        "\n",
        "Definition: These variables can have any value within a range, and the difference between two values is meaningful.\n",
        "\n",
        "Examples: Height, weight, temperature, income, or time.\n",
        "\n",
        "\n",
        "###Categorical Variables:\n",
        "Definition:\n",
        "These variables represent categories or groups, and each observation belongs to only one category.\n",
        "\n",
        "Examples:\n",
        "Gender (male/female), color (red/blue/green), or type of car (sedan/SUV/truck)."
      ],
      "metadata": {
        "id": "ygyoalZIIDB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "--> Handling categorical variables in machine learning involves converting them into a numerical format that algorithms can understand.\n",
        "\n",
        "Common techniques include \"one-hot encoding, label encoding, ordinal encoding, and frequency encoding\". These methods transform categorical features into numerical representations, enabling machine learning models to process and utilize them effectively."
      ],
      "metadata": {
        "id": "xgYDDQrzIdfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.7.What do you mean by training and testing a dataset?\n",
        "\n",
        "-->\n",
        "###Training Dataset:\n",
        "This dataset is used to adjust the model's parameters (like weights in a neural network) so it can learn patterns and make accurate predictions. The model is repeatedly exposed to the training data, and its performance is monitored as it learns.\n",
        "\n",
        "\n",
        "###Testing Dataset:\n",
        "This dataset is used to assess the model's performance on data it hasn't seen during training. It's crucial to ensure that the testing data is independent of the training data to avoid bias. By comparing the model's predictions on the testing dataset with the actual values, you can determine how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "NHUQZJyOIzlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.8.What is sklearn.preprocessing?\n",
        "\n",
        "--> The sklearn.preprocessing module in scikit-learn provides a collection of tools and techniques for transforming raw data into a format suitable for machine learning algorithms. This module is essential because raw data often needs to be preprocessed before it can be effectively used to train machine learning models."
      ],
      "metadata": {
        "id": "w0lNsAneJDFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.9.What is a Test set?\n",
        "\n",
        "--> A test set is a portion of a dataset used to evaluate the performance of a machine learning model after it has been trained. It's separate from the training data and provides an unbiased assessment of how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "8DcCbqfpJKvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.10.How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "--> In Python, data is typically split for model fitting by using the train_test_split function from the scikit-learn library. A machine learning problem is approached through a series of steps, including data collection, preparation, model selection, training, evaluation, and parameter tuning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Approaching a Machine Learning Problem:\n",
        "\n",
        "1.Define the problem:\n",
        "Clearly articulate the business problem and what you want to achieve.\n",
        "\n",
        "2.Collect data: Gather the necessary data for your problem.\n",
        "\n",
        "3.Prepare the data: Clean, transform, and preprocess the data.\n",
        "\n",
        "4.Choose a model: Select an appropriate machine learning algorithm based on your problem and data.\n",
        "\n",
        "5.Train the model: Use the training data to train the chosen model.\n",
        "\n",
        "6.Evaluate the model: Assess the performance of the model using the testing data.\n",
        "\n",
        "7.Tune parameters: Optimize the model's parameters to improve performance.\n",
        "\n",
        "8.Make predictions: Use the trained model to make predictions on new data."
      ],
      "metadata": {
        "id": "yiHPza9zJaNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.11.Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "--> Exploratory Data Analysis (EDA) before model fitting is crucial for several reasons. It allows for a better understanding of the data's structure, helps identify potential issues like outliers and missing values, and facilitates the selection of appropriate modeling techniques."
      ],
      "metadata": {
        "id": "uxi3QwQuKAsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.12.What is correlation?\n",
        "\n",
        "--> Correlation is a statistical measure that assesses the strength and direction of the relationship between two variables.\n",
        "\n",
        "It indicates how much two variables are associated with each other.\n",
        "\n",
        "Correlation does not imply causation; just because two variables are correlated doesn't mean one causes the other."
      ],
      "metadata": {
        "id": "5TcjZSa-KINh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.13.What does negative correlation mean?\n",
        "\n",
        "--> A negative correlation, also known as an inverse correlation, means that as one variable increases, the other variable decreases, and vice versa. Essentially, the two variables move in opposite directions."
      ],
      "metadata": {
        "id": "yqltGZNpKUZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.14.How can you find correlation between variables in Python?\n",
        "\n",
        "-->The correlation between variables in Python using libraries like pandas, NumPy, or SciPy. The most common method is using Pearson correlation, which measures linear relationships."
      ],
      "metadata": {
        "id": "fud8m_g-Kg47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.15.What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "-->\n",
        "###Causation:\n",
        "Causation:\n",
        "A cause-and-effect relationship where one event directly leads to another. For example, hitting a button on a remote control causes the television to turn on.\n",
        "\n",
        "| Aspect         | Correlation                             | Causation                                   |\n",
        "| -------------- | --------------------------------------- | ------------------------------------------- |\n",
        "| Definition     | A mutual relationship between variables | One variable directly affects another       |\n",
        "| Directionality | No implied direction                    | Clear direction: cause → effect             |\n",
        "| Proof          | Statistical association                 | Requires experimentation or strong evidence |\n",
        "| Example        | Ice cream sales and drowning rates      | Smoking causes lung cancer                  |\n"
      ],
      "metadata": {
        "id": "CAcaRVcYKvpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "-->\n",
        "###Optimizer:\n",
        "An optimizer in machine learning is an algorithm that adjusts model parameters (like weights and biases) during training to minimize the loss function. It's essentially a method for finding the optimal settings for a model's parameters to make the most accurate predictions.\n",
        "\n",
        "###Types of Optimizer:\n",
        "####1. Gradient Descent:\n",
        "Updates weights after computing gradients on the entire dataset. This method is stable and accurate but can be computationally expensive for large datasets.\n",
        "\n",
        "Example: Training a simple linear regression model with a large dataset.\n",
        "\n",
        "####2.Stochastic Gradient Descent (SGD):\n",
        "\n",
        "Updates weights after each training example. This method can be faster and escape local minima more effectively but can also be less stable than batch gradient descent.\n",
        "\n",
        "Example:\n",
        "Training a logistic regression model with a massive dataset where memory constraints limit batch size.\n",
        "\n",
        "####3. Mini-batch Gradient Descent:\n",
        "\n",
        "A compromise between batch and stochastic gradient descent, using a subset of the data (mini-batch) to compute gradients. This method is often used in practice as a balance between speed and stability.\n",
        "\n",
        "Example:\n",
        "Training a deep neural network for image classification, where a batch size of 32 or 64 is commonly used.\n",
        "\n",
        "####4.Momentum:\n",
        "Accelerates SGD by adding a fraction of the previous update to the current one. This helps the algorithm move faster in the correct direction and can escape local minima more effectively.\n",
        "\n",
        "Example:\n",
        "Training a neural network for image classification, where the model is prone to getting stuck in local minima.\n",
        "\n",
        "####5.Nesterov Accelerated Gradient (NAG):\n",
        "A refined version of Momentum that looks ahead before updating. This can lead to more precise and responsive updates.\n",
        "\n",
        "Example:\n",
        "Training a neural network for natural language processing, where the model needs to be very precise and responsive."
      ],
      "metadata": {
        "id": "6Ez8dGsqLVGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.17.What is sklearn.linear_model ?\n",
        "\n",
        "--> sklearn.linear_model is a module within the scikit-learn (sklearn) library in Python that provides a variety of linear models for regression and classification tasks. Linear models assume a linear relationship between the input features and the target variable."
      ],
      "metadata": {
        "id": "7112QOyfMY4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.18.What does model.fit() do? What arguments must be given?\n",
        "\n",
        "--> The model.fit() function in machine learning is used to train a model on a given dataset. It adjusts the model's internal parameters (weights and biases) to minimize a loss function, using optimization techniques like gradient descent. This process is essential for the model to learn patterns from the data, allowing it to make predictions on new, unseen data.\n",
        "\n",
        "Key Arguments:\n",
        "\n",
        "1.Training Data (x)\n",
        "\n",
        "2.Target Data (y)\n",
        "\n",
        "3.batch_size\n",
        "\n",
        "4.epochs\n",
        "\n",
        "5.validation_data\n",
        "\n",
        "6.shuffle\n",
        "\n",
        "7.callbacks\n",
        "\n",
        "8.verbose\n",
        "\n",
        "9.loss\n",
        "\n",
        "10.optimizer\n",
        "\n",
        "11.metrics"
      ],
      "metadata": {
        "id": "7R8d1PZZMf30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "--> model. predict() is used to generate predictions from the trained model based on new input data. It does not require true labels and does not compute any metrics.\n",
        "\n",
        "Key arguments:\n",
        "\n",
        "1.batch_size:\tNumber of samples per batch for prediction.\n",
        "\n",
        "2.verbose:\t0, 1, or 2 for silence, progress bar, or one line per batch\n",
        "\n",
        "3.steps:\tTotal number of batches before stopping (used with generators)\n",
        "\n",
        "4.callbacks:\tOnly in newer versions (seldom used)"
      ],
      "metadata": {
        "id": "RnSp0chENRTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.20.What are continuous and categorical variables?\n",
        "\n",
        "-->\n",
        "####Continuous Variables:\n",
        "These variables can have any value within a range, and the difference between two values is meaningful.\n",
        "\n",
        "Examples: Height, weight, temperature, income, or time.\n",
        "\n",
        "####Categorical Variables:\n",
        "These variables represent categories or groups, and each observation belongs to only one category.\n",
        "\n",
        "Examples: Gender (male/female), color (red/blue/green), or type of car (sedan/SUV/truck)."
      ],
      "metadata": {
        "id": "ggreCMC-OGnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.21.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "-->\n",
        "####Feature Scaling:\n",
        "Feature scaling in machine learning is the process of transforming the values of numerical features in a dataset to a common scale or range. This is crucial for algorithms that are sensitive to the scale of data, as it prevents features with larger magnitudes from disproportionately influencing the model's performance.\n",
        "\n",
        "\n",
        "Feature scaling in machine learning aims to normalize the range of independent variables or features in a dataset, ensuring they all contribute equally to the learning process. This prevents features with larger scales from dominating the model's performance and improves the efficiency and accuracy of various machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "ZfW_6qEgOUkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.22.How do we perform scaling in Python?\n",
        "\n",
        "--> Feature scaling is a preprocessing step where you normalize or standardize the range of independent variables to ensure that each feature contributes equally to model training.\n",
        "\n",
        "1.  Standardization (Z-score Scaling):\n",
        "\n",
        "i. Centers features around mean = 0, with standard deviation = 1.\n",
        "\n",
        "ii. Use when features have normal distributions.\n",
        "\n",
        "2. Min-Max Scaling:\n",
        "\n",
        "i. Scales features to a [0, 1] range.\n",
        "\n",
        "ii. Use when you want bounded features.\n",
        "\n",
        "3. Robust Scaling:\n",
        "\n",
        "i. Uses median and IQR (interquartile range), so it's robust to outliers.\n",
        "\n",
        "4. MaxAbs Scaling:\n",
        "\n",
        "i. Scales each feature by its maximum absolute value. Keeps sparsity in data."
      ],
      "metadata": {
        "id": "lXU9Xf28Oj7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.23. What is sklearn.preprocessing?\n",
        "\n",
        "--> The sklearn.preprocessing module in scikit-learn provides a collection of tools and techniques for transforming raw data into a format suitable for machine learning algorithms. This module is essential because raw data often needs to be preprocessed before it can be effectively used to train machine learning models."
      ],
      "metadata": {
        "id": "tU9FnGGePJiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "--> In Python, data is typically split for model fitting by using the train_test_split function from the scikit-learn library. A machine learning problem is approached through a series of steps, including data collection, preparation, model selection, training, evaluation, and parameter tuning."
      ],
      "metadata": {
        "id": "QNnRCotPPS9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q.25. Explain data encoding?\n",
        "\n",
        "--> Data encoding is the process of converting information from one form to another, often for efficient transmission, storage, or analysis. It essentially involves transforming data into a specific format that can be easily processed or understood by computers or other systems."
      ],
      "metadata": {
        "id": "uDW5_SdBPeG4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cy07K8DEw3H"
      },
      "outputs": [],
      "source": []
    }
  ]
}